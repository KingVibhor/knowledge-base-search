# Knowledge-Based RAG Engine (Gemini + ChromaDB)

A lightweight Retrieval-Augmented Generation (RAG) system that lets you upload
documents (PDF, DOCX, TXT, images) and ask questions about them using Google
Gemini models. The system extracts text, embeds it using Gemini embeddings,
stores vectors in ChromaDB, retrieves relevant chunks, and generates an answer
using Gemini 2.0 Flash.

This project is designed to be simple, fast, and easy to extend.

---

## Features

- Upload PDFs, DOCX, TXT, and Images
- Text extraction + OCR fallback (pdfplumber + Tesseract)
- Chunking and metadata storage
- Gemini-powered embeddings (text-embedding-004)
- ChromaDB vector store (persistent)
- RAG pipeline with Gemini Flash for answer generation
- Clean frontend UI (HTML + JS)
- Fully CORS-enabled backend (FastAPI)
- Works entirely locally (except Gemini API calls)

---

## Tech Stack

### **Backend**
- Python 3.10
- FastAPI
- ChromaDB (Persistent DB)
- Google Generative AI (Gemini APIs)
- pdfplumber (PDF extraction)
- pytesseract + pdf2image (OCR fallback)
- DOCX parser
- Uvicorn server

### **Frontend**
- HTML / CSS / JavaScript
- Modern, clean minimalistic interface
- AJAX file upload + query system
- Displays answer and sources

---

## ğŸ“ Project Structure

```
knowledge-based-rag/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py               # FastAPI server + routes
â”‚   â”œâ”€â”€ embeddings.py        # Gemini embedding functions
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB setup + CRUD
â”‚   â”œâ”€â”€ rag_pipeline.py      # Extraction, chunking, ingestion
â”‚   â”œâ”€â”€ llm_client.py        # Gemini answer synthesis
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html           # Main UI
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ docs/                # Uploaded documents
â”‚   â””â”€â”€ vectorstore/         # ChromaDB persistent DB
â”‚
â””â”€â”€ README.md
```

## Installation & Setup

### 1ï¸ **Install Python 3.10**  
Your system must use **Python 3.10** because newer Python versions break several dependencies.

### 2ï¸ Create a virtual environment
cd knowledge-based-rag
python -m venv .venv

makefile
Copy code

Activate:

Windows (PowerShell):
.venv\Scripts\activate

shell
Copy code

### 3ï¸ Install dependencies
pip install -r requirements.txt


### 4ï¸ Install Tesseract (for image-only PDFs)
Windows:
- Download: https://github.com/UB-Mannheim/tesseract/wiki
- Install
- Add to PATH:
C:\Program Files\Tesseract-OCR\

Verify:
tesseract --version

### 5ï¸ Set your Gemini API key
set GEMINI_API_KEY "YOUR_KEY"

OR create `.env` with:
GEMINI_API_KEY=YOUR_KEY

---

##  Run Backend

uvicorn backend.app:app --reload

Backend runs on:
http://127.0.0.1:8000

---

##  Run Frontend

cd frontend
python -m http.server 5500

Visit:
http://127.0.0.1:5500/index.html

---

## Usage Flow

### 1. Upload Document  
Click **Upload**, select a PDF/DOCX/TXT/Image â†’ backend extracts text â†’ chunks â†’ embeds â†’ stores.

### 2. Ask a Question  
Enter query â†’ backend retrieves relevant chunks â†’ Gemini generates final answer.

### 3. See Output  
UI shows:
- Final Answer
- Sources (file + chunk index)
- Extracted text snippet

---

## Notes

- Works well with real-world messy PDFs.
- Supports OCR auto fallback for scanned/image-only files.
- ChromaDB persists vectors automatically.
- Fully stateless â€” restart server without losing data.

---

## Future Improvements
- Support multiple collection namespaces
- Add UI for viewing ingested documents
- Add PDF preview before ingestion
- Add Docker support

---

## License
MIT License